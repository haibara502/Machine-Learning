\documentclass{article}

\usepackage{graphicx}
\usepackage{float}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage{caption}

\usepackage{amssymb}
\newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
\newenvironment{guess}[1]{\par\noindent\underline{Guess:}\space#1}{}
\usepackage{amsthm}

\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm}

\begin{document}
	\section{Warm up: Linear Classifiers and Boolean Functions}
		\begin{enumerate}
			\item It is linearly separable. The linear threshold unit could be:
			\begin{equation}
				y = \left\{ \begin{array}{ll}
					1 & if \quad x_1 + x_3 - x_2 \geq 0 \\
					0 & otherwise \end{array} \right.
			\end{equation}
			\item It is linearly separable. The linear threshold unit could be:
			\begin{equation}
				y = \left\{ \begin{array}{ll}
					1 & if \quad x_1 + x_3 + 2x_2 \geq 2 \\
					0 & otherwise \end{array} \right.
			\end{equation}
			\item It is not linearly separable.
			\item It is not linearly separable.
			\item It is linearly separable. The linear threshold unit could be:
			\begin{equation}
				y = \left\{ \begin{array}{ll}
					1 & if \quad -x_1+x_2-x_3\geq 1 \\
					0 & otherwise \end{array} \right.
			\end{equation}
		\end{enumerate}
	\section{Mistake Bound Model of Learning}
	\begin{enumerate}
		
		\item \begin{enumerate} \item There are only $80$ possible values for $l$. And each $l$ corresponds to a unique function in the concept class. So we can se that 
		\begin{equation}
			|C| = 80
		\end{equation}
		\item Define a function
		\begin{equation}
			g(x_1, x_2) = f_l(x_1,x_2) - y^t
		\end{equation}
		if $g()$ equals zero, no mistake is made here. Otherwise, it makes a mistake.
		\item \begin{enumerate}
		\item If $g(x_1, x_2)$ is greater than zero, showing that function $f$ think it is positive but the true label isn't. In this case, the range of $f$ is bigger than we want. So remove all the functions with length no smaller than $l$.
		\item If $g(x_1, x_2)$ is smaller than zero, showing that function $f$ thinks that it is negative but the true label is positive. In this case, the range of $l$ is smaller than what we want. So remove all the functions whose length is no bigger than $l$ in the concept class.
		\end{enumerate}
		\item \begin{algorithm}[H]
		\caption{Mistake-driven Learning Algorithm}
		\begin{algorithmic}[1]
			\State Initialize $C_0 = C$.
			\State \For{ sample $(x_1^i, x_2^i)$}
				\State Find function $f_k$ whose length $k$ is the middle number among the length of all the functions in current concept class $C_i$.
				\State Check if the function $f_k$ made a mistake.
				\State \If{$f_k$ doesn't make a mistake}
					\State Assign $C_{i + 1} = C_{i}$
					\State Continue to the next sample
				\Else { 
					\State Calculate $g = f_k(x_1^i, x_2^i) - y^i$
					\If {$g > 0$}
						\State Remove all the functions in $C_i$ with length $\geq k$.
					\Else {
						\State Remove all the functions in $C_i$ with length $\leq k$.
					\EndIf}
				} \EndIf
				\If {$|C_i| = 1$}
					\State Break the loop.
				\EndIf
				\State Assign $C_{i + 1} = C_i$
            \EndFor
			
			\State \Return the only remaining function.
        \end{algorithmic}
        \end{algorithm}
		\end{enumerate}
	\item \begin{proof}
		For the Halving algorithm, the algorithm will stop when only $M$ functions in the concept class. Because the remaining  $M$ functions are all perfect experts since they will never be removed from the concept class. Suppose the algorithm made $k$ mistakes before it stops. Then we have:
		\begin{equation}
			M \times 2^k = N
		\end{equation}
		So we know that $k = \log{\frac{N}{M}}$. It means that the mistakes the algorithm will make is no bigger than $\log{\frac{N}{M}}$. That is, the mistake bound of it is $O(\log{\frac{N}{M}})$.
	\end{proof}
	\end{enumerate}
	\section{The perceptron Algorithm and its Variants}
	\begin{enumerate}
		\item The whole program is realized using \emph{$c++$}. To realize it, I divide the program into several parts. The main idea is that, I want to encapsulate the procedure inside different classes to simplify the overall complexity of the code. Also, by doing this, the way to reuse the classes are convinient. The first part is called \emph{url}. It handles all the examples. It stores the label and the feature vector of the example and will return corresponding value during query. The second part is called \emph{urls}. This class is used to handle all of the examples. It has two(three when using the dev data) vectors corresponding to the training set and the test data set separately. In each training time, it will fed the algorithm the examples and return required values. The third part is called \emph{perceptron}. This class is used to calculate the training and test(also dev process in dev dataset) datasets. By fedding it the examples, it will train the perceptron. And by giving the examples, it can return the testing accuracy. That is to say, I encapsulate all the training and testing process inside this class. The final part is the \emph{main c++ file}. It first initialize the perceptron and the urls. Then it reads the required hyperparameters. And it will also control the epoch of the training and output the accuracy. 
		\begin{description}
			\item[url]
				This is class is used to inclose all the information of an example. By defining this, I can store the feature and the label inside this class and call the corresponding functions to get the value when needed.
			\item[urls]
				This class is used to handle all the examples. It will store the training and the testing examples in two vectors. Everytime the training process starts, just call its function and it can fed the data one by one. By doing this, I can using the whole data when needed and don't need to define any global variables to remember the data.
			\item[perceptron]
				This class imitate the process of the perceptron. It contains the weight vector and the bias. These values are randomly initialized at the beginning. After that, everytime just give the dataset to it and specify which process to do, it can finish the process itself and then return the answer. By doing this, the processes are only occured inside this class thus when doing little changes to the perceptron, it is very convinient. It contains the training process, which will do the update of the weight and bias. It also contains the test process, which will return the accuracy of the testset.
			\item[main program] The main program mainly used to combine the parts above into a program. It first initialize one \emph{urls} and one \emph{perceptron}. And then it will call the \emph{urls} to read the data. After that, it controls the epoch, and in each epoch, giving the examples to the \emph{perceptron} and require the needed value. At the end of the program, return the accuracy of the testset by calling the \emph{perceptron} to do that.
		\end{description}
		\item For the test and development set, the accuracy must $\geq 50\%$. And more specificly, suppose there are total $N$ examples and there are $M$ examples with the most frequent label. Then the accuracy would be $N / M \times 100\%$.
		\item \begin{tabular}{|c|c|c|c|c|c|}
		\hline
			Algorithm & Bst hypr-pam & CV accuracy & \# Updates & Dev accuracy & Test accuracy \\
		\hline
			Basic Perceptron & 0.1 & 0.914603 & 13101 & 0.904486 & 0.912446\\
		\hline
			Dynamic Perceptron & 1 & 0.92908 & 10993 & 0.913169 & 0.922576\\
		\hline
			Margin Perceptron & 0.1 , 1& 0.932699 & 18571 & 0.92547 & 0.929088\\
		\hline 
			Average Perceptron & 1 & 0.930889 & 25198 & 0.928365 & 0.931259 \\
		\hline
			Aggresive Perceptron & 0.1  & 0.916659 & 27365 & 0.895803 & 0.900145\\
		\hline
		\end{tabular}
		\begin{description}
			\item[Simple perceptron]
				\includegraphics[width=0.5\textwidth]{simple1.png}
			\item[Dynamic perceptron]
				\includegraphics[width=0.5\textwidth]{dynamic.png}
			\item[Margin perceptron]
				\includegraphics[width=0.5\textwidth]{margin1.png}
			\item[Average perceptron]
				\includegraphics[width=0.5\textwidth]{average.png}
			\item[Aggressive perceptron]
				\includegraphics[width=0.5\textwidth]{aggresive1.png}
		\end{description}
	\end{enumerate}
\end{document}
